{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import json, pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rescale\n",
    "\n",
    "from AxonDeepSeg.apply_model import axon_segmentation\n",
    "from AxonDeepSeg.testing.segmentation_scoring import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to apply each model to the test images to compute the statistics we are going to use to compare the models. \n",
    "It reads an existing json model_comparison file in order not to have to relaunch each model each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most important part: define where to find the models to test as well as the data to test on\n",
    "path_models = '../test_models/'\n",
    "path_testing = '../data/baseline_testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reminder: the more resolution we have the less rescaling we have to do (thus the smaller the gps is!)\n",
    "\n",
    "gps_dict = {\n",
    "    'SEM_3c_256':0.1,\n",
    "    'SEM_3c_512':0.1, #Because not same semantics as SEM_3c_256 here\n",
    "    'TEM_3c_256':0.02,\n",
    "    'TEM_3c_512':0.01,\n",
    "    'TEM_3c_1024':0.005\n",
    "}\n",
    "\n",
    "stats = {\n",
    "    'best_acc_model':'best_acc_stats',\n",
    "    'best_loss_model':'best_loss_stats',\n",
    "    'model':'evolution_stats'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parameters for the segmentation, like the smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_value = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful function to transform a png mask to a mask with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labellize(mask_raw, thresh = [0, 0.2, 0.8]):\n",
    "    max_ = np.max(mask_raw)\n",
    "    n_c = len(thresh)\n",
    "    \n",
    "    mask = np.zeros_like(mask_raw)\n",
    "    for i, e in enumerate(thresh[1:]):\n",
    "        mask[np.where(mask_raw >= e*255)] = i+1\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(mask_raw):\n",
    "    vals = np.unique(mask_raw)\n",
    "    mask = np.zeros((mask_raw.shape[0], mask_raw.shape[1], len(vals)))\n",
    "    for i,e in enumerate(vals):\n",
    "        mask[:,:,i] = mask_raw == e\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't want to create an image, but just return the predictions (not the probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for folder in os.listdir(path_models): # Loop over all models\n",
    "    path_model = os.path.join(path_models, folder)\n",
    "    if os.path.isdir(path_model):        \n",
    "        model_comparison_list = []\n",
    "        # First we have to retrieve some essential data from the config file\n",
    "        with open(os.path.join(path_model,'config_network.json'), 'r') as fd:\n",
    "            config_network = json.loads(fd.read())\n",
    "        trainingset_name = config_network['network_trainingset']\n",
    "        type_trainingset = trainingset_name.split('_')[0]\n",
    "        n_classes = config_network['network_n_classes']\n",
    "        # We are now going to loop over all checkpoint files.\n",
    "        for checkpoint in os.listdir(path_model):\n",
    "            if checkpoint[-10:] == '.ckpt.meta':\n",
    "                result_model = {}\n",
    "                name_checkpoint = checkpoint[:-10]\n",
    "                result_model.update({'id_model':folder,\n",
    "                                    'ckpt':name_checkpoint,\n",
    "                                    'config':config_network})\n",
    "                \n",
    "                # First we compute the training statistics, which are independent of the testing images\n",
    "                \n",
    "                # >> Validation 10-moving average statistics\n",
    "                try:                        \n",
    "                    f = open(path_model + '/'+ stats['name_checkpoint'] + '.pkl', 'r')\n",
    "                    res = pickle.load(f)\n",
    "                    acc_stats = res['accuracy']\n",
    "                    loss_stats = res['loss']\n",
    "                    epoch_stats = res['steps']\n",
    "                except:\n",
    "                    print 'No stats file found...'\n",
    "                    f = open(path_model + '/evolution.pkl', 'r')\n",
    "                    res = pickle.load(f)\n",
    "                    epoch_stats = max(res['steps'])\n",
    "                    acc_stats = np.mean(res['accuracy'][-10:])\n",
    "                    loss_stats = np.mean(res['loss'][-10:])\n",
    "\n",
    "                result_model.update({'training_stats':{\n",
    "                    'training_epoch':epoch_stats,\n",
    "                    'training_mvg_avg10_acc':acc_stats,\n",
    "                    'training_mvg_avg10_loss':loss_stats\n",
    "                                                       },\n",
    "                                     'testing_stats':[]\n",
    "                                    })\n",
    "                \n",
    "                \n",
    "                # We now want to test on each SEM and TEM image\n",
    "                # We use each folder of the baseline_testing in data. They have a prefix depending if SEM or TEM\n",
    "                \n",
    "                for testing_folder in tqdm(os.listdir(path_testing)):\n",
    "                    path_testing_folder = os.path.join(path_testing, testing_folder)\n",
    "                    if os.path.isdir(path_testing_folder):\n",
    "\n",
    "                        # Reading the images and processing them if needed\n",
    "                        mask_raw = imread(os.path.join(path_testing_folder, 'mask.png'), flatten=True, mode='L')\n",
    "                        img_raw = imread(os.path.join(path_testing_folder, 'image.png'), flatten=True, mode='L')\n",
    "                        mask = labellize(mask_raw)\n",
    "                        \n",
    "                        # We infer the name of the different files\n",
    "                        type_image = testing_folder.split('_')[0] # SEM or TEM\n",
    "                        name_image = '_'.join(testing_folder.split('_')[1:]) # Rest of the name of the image\n",
    "                        testing_stats_dict = {'type_image':type_image, \n",
    "                                              'name_image':name_image}\n",
    "                        \n",
    "                        # Choosing the gps\n",
    "                        name_gps = type_image + '_3c_' + str(trainingset_name.split('_')[-1])\n",
    "                        gps = gps_dict[name_gps]\n",
    "                        \n",
    "                        '''\n",
    "                        print 'GPS chosen: ' + str(gps)\n",
    "                        file = open(path_testing_folder + '/pixel_size_in_micrometer.txt', 'r')\n",
    "                        pixel_size = float(file.read())\n",
    "                        plt.figure()\n",
    "                        plt.imshow(rescale(img_raw, float(pixel_size)/gps, preserve_range=True)[0:256,0:256], cmap='gray')\n",
    "                        plt.colorbar()\n",
    "                        plt.show();\n",
    "                        '''\n",
    "\n",
    "                        # Making the prediction\n",
    "                        print 'Beginning segmentation of ' + type_image + ' image with ' + type_trainingset + ' model, id ' + str(folder) + '...'\n",
    "                        prediction, pred_proba = axon_segmentation(path_testing_folder, \n",
    "                                                       path_model, \n",
    "                                                       config_network,\n",
    "                                                       ckpt_name = name_checkpoint,\n",
    "                                                       crop_value=crop_value, \n",
    "                                                       general_pixel_size=gps,\n",
    "                                                       pred_proba=True,\n",
    "                                                       write_mode=False\n",
    "                                                      )\n",
    "                        \n",
    "                        print 'Statistics extraction...'\n",
    "                        # Processing pred_proba into statistics\n",
    "                        a = np.exp(pred_proba)\n",
    "                        b = np.sum(a, axis=-1)\n",
    "                        pred_proba = np.stack([np.divide(a[:,:,i],b) for i in range(n_classes)], axis=-1)\n",
    "                        \n",
    "                        # Computation of metrics\n",
    "                        vec_prediction = np.reshape(binarize(prediction), (-1,n_classes))\n",
    "                        vec_pred_proba = np.reshape(pred_proba, (-1,n_classes))\n",
    "                        vec_mask = np.reshape(binarize(mask), (-1,n_classes))\n",
    "                        # >> Accuracy and XEntropy loss\n",
    "                        testing_stats_dict.update({\n",
    "                            'accuracy':accuracy_score(mask.ravel(), prediction.ravel()),\n",
    "                            'log_loss':log_loss(vec_mask, vec_pred_proba)\n",
    "                                                   })\n",
    "                        # >> Pixel wise dice, both classes, and element wise dice\n",
    "                        gt_axon = binarize(mask)[:,:,-1]\n",
    "                        pred_axon = binarize(prediction)[:,:,-1]\n",
    "                        pw_dice_axon = pw_dice(pred_axon, gt_axon)\n",
    "                        testing_stats_dict.update({\n",
    "                            'pw_dice_axon':pw_dice_axon})\n",
    "\n",
    "                        '''\n",
    "                        ew_sensitivity_axon, ew_precision_axon, ew_diffusion_axon = score_analysis(img_raw, \n",
    "                                                                                          gt_axon, \n",
    "                                                                                          pred_axon)\n",
    "\n",
    "                        data_axon_dice = dice(img_raw, gt_axon, pred_axon, min_area=4)\n",
    "                        ew_dice_mean_axon = data_axon_dice['dice'].mean()\n",
    "                        ew_dice_quant_axon = data_axon_dice['dice'].quantile([0.1, 0.5, 0.9, 0.95])\n",
    "\n",
    "                        result_model.update({name_testing_image:{\n",
    "                            'ew_dice_mean_axon':ew_dice_mean_axon,\n",
    "                            'ew_dice_quant10_axon':ew_dice_quant_axon.values[0],\n",
    "                            'ew_dice_quant50_axon':ew_dice_quant_axon.values[1],\n",
    "                            'ew_dice_quant90_axon':ew_dice_quant_axon.values[2],\n",
    "                            'ew_dice_quant95_axon':ew_dice_quant_axon.values[3],\n",
    "                            'ew_sentivity_axon':ew_sensitivity_axon,\n",
    "                            'ew_precision_axon':ew_precision_axon,\n",
    "                            'ew_diffusion_axon':ew_diffusion_axon\n",
    "                                                              }\n",
    "                                             })\n",
    "                        '''\n",
    "                        if n_classes == 3:\n",
    "                            gt_myelin = binarize(mask)[:,:,1]\n",
    "                            pred_myelin = binarize(prediction)[:,:,1]\n",
    "                            pw_dice_myelin = pw_dice(pred_myelin, gt_myelin)\n",
    "                            \n",
    "                            testing_stats_dict.update({\n",
    "                                'pw_dice_myelin':pw_dice_myelin})\n",
    "                            \n",
    "                        result_model['testing_stats'].append(testing_stats_dict)\n",
    "                            \n",
    "                model_comparison_list.append(result_model)               \n",
    "        \n",
    "        # Finally we save the model in a new json file.\n",
    "        path_file = os.path.join(path_models, 'model_comparison.json')\n",
    "        if os.path.exists(path_file):\n",
    "            with open(path_file, 'r') as fd:\n",
    "                existing_dict = json.loads(fd.read())\n",
    "        else:\n",
    "            existing_dict = {'data':[]}\n",
    "\n",
    "        existing_dict['data'].append(model_comparison_list)\n",
    "\n",
    "        with open(path_file, 'w') as f:\n",
    "            json.dump(existing_dict, f, indent=2)\n",
    "        model_comparison_list = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the json file into an Excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
