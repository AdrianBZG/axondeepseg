{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-DEFINING USEFUL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## HEADER ##########\n",
    "# Config file description :\n",
    "\n",
    "# network_learning_rate : float : No idea, but certainly linked to the back propagation ? Default : 0.0005.\n",
    "\n",
    "# network_n_classes : int : number of labels in the output. Default : 2.\n",
    "\n",
    "# network_dropout : float : between 0 and 1 : percentage of neurons we want to keep. Default : 0.75.\n",
    "\n",
    "# network_depth : int : number of layers WARNING : factualy, there will be 2*network_depth layers. Default : 6.\n",
    "\n",
    "# network_convolution_per_layer : list of int, length = network_depth : number of convolution per layer. Default : [1 for i in range(network_depth)].\n",
    "\n",
    "# network_size_of_convolutions_per_layer : list of lists of int [number of layers[number_of_convolutions_per_layer]] : Describe the size of each convolution filter.\n",
    "# Default : [[3 for k in range(network_convolution_per_layer[i])] for i in range(network_depth)].\n",
    "\n",
    "# network_features_per_layer : list of lists of int [number of layers[number_of_convolutions_per_layer[2]] : Numer of different filters that are going to be used.\n",
    "# Default : [[64 for k in range(network_convolution_per_layer[i])] for i in range(network_depth)]. WARNING ! network_features_per_layer[k][1] = network_features_per_layer[k+1][0].\n",
    "\n",
    "# network_trainingset : string : describe the trainingset for the network.\n",
    "\n",
    "# network_downsampling : string 'maxpooling' or 'convolution' : the downsampling method.\n",
    "\n",
    "# network_thresholds : list of float in [0,1] : the thresholds for the ground truthes labels.\n",
    "\n",
    "# network_weighted_cost : boolean : whether we use weighted cost for training or not.\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default_config():\n",
    "    \"\"\"Generate the default config dict\"\"\"\n",
    "    network_learning_rate = 0.0005\n",
    "    network_n_classes = 3\n",
    "    dropout = 0.75\n",
    "    network_depth = 6\n",
    "    network_convolution_per_layer = [3 for i in range(network_depth)]\n",
    "    network_size_of_convolutions_per_layer = [[5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5], [5, 5, 5]]  \n",
    "    network_features_per_convolution =  [[[1, 10], [10, 20], [20, 30]], [[30, 40], [40, 50], [50, 60]],\n",
    "                                        [[60, 70], [70, 80], [80, 90]], [[90, 100], [100, 110], [110, 120]],\n",
    "                                        [[120, 130], [130, 140], [140, 150]], [[150, 160], [160, 170], [170, 180]]]\n",
    "    trainingset = 'simulation_png'\n",
    "    downsampling = 'maxpooling'\n",
    "    thresholds = [0, 0.1, 0.8]\n",
    "    weighted_cost = False\n",
    "    batch_size = 1\n",
    "\n",
    "    config = {\n",
    "        'network_learning_rate': network_learning_rate,\n",
    "        'network_n_classes': network_n_classes,\n",
    "        'network_dropout': dropout,\n",
    "        'network_depth': network_depth,\n",
    "        'network_convolution_per_layer': network_convolution_per_layer,\n",
    "        'network_size_of_convolutions_per_layer': network_size_of_convolutions_per_layer,\n",
    "        'network_features_per_convolution': network_features_per_convolution,\n",
    "        'network_trainingset': trainingset,\n",
    "        'network_downsampling': downsampling,\n",
    "        'network_thresholds': thresholds,\n",
    "        'network_weighted_cost': weighted_cost,\n",
    "        'network_batch_size': batch_size\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(depth,network_first_num_features,features_augmentation,network_convolution_per_layer):\n",
    "\n",
    "    increment = int(float(features_augmentation[1:]))\n",
    "\n",
    "    if str(features_augmentation[0]) == 'p':\n",
    "        # Add N features at each convolution layer.\n",
    "        first_conv = [[1,network_first_num_features]]\n",
    "        temp = [[network_first_num_features+i*increment,network_first_num_features+(i+1)*increment] \n",
    "                                for i in range(network_convolution_per_layer[0])[1:]]\n",
    "        first_layer = first_conv + temp\n",
    "        last_layer = first_layer\n",
    "        network_features_per_convolution = [first_layer]\n",
    "\n",
    "        for cur_depth in range(depth)[1:]:\n",
    "\n",
    "            first_conv = [[last_layer[-1][-1],last_layer[-1][-1]+increment]]\n",
    "            temp = [[last_layer[-1][-1]+i*increment,last_layer[-1][-1]+(i+1)*increment] for i in range(network_convolution_per_layer[cur_depth])[1:]]\n",
    "            current_layer = first_conv+temp\n",
    "            network_features_per_convolution = network_features_per_convolution + [current_layer]\n",
    "\n",
    "            last_layer = current_layer\n",
    "\n",
    "    elif str(features_augmentation[0]) == 'x':\n",
    "        # Multiply the number of features by N at each \"big layer\".\n",
    "        \n",
    "        first_conv = [[1,network_first_num_features]]\n",
    "        temp = [[network_first_num_features,network_first_num_features] \n",
    "                                for i in range(network_convolution_per_layer[0]-1)]\n",
    "        first_layer = first_conv + temp\n",
    "        last_layer = first_layer\n",
    "        network_features_per_convolution = [first_layer]\n",
    "        for cur_depth in range(depth)[1:]:\n",
    "            first_conv = [[last_layer[-1][-1],last_layer[-1][-1]*increment]]\n",
    "            temp = [[last_layer[-1][-1]*increment,last_layer[-1][-1]*increment] for i in range(network_convolution_per_layer[cur_depth]-1)]\n",
    "            current_layer = first_conv+temp\n",
    "            network_features_per_convolution = network_features_per_convolution + [current_layer]\n",
    "\n",
    "            last_layer = current_layer\n",
    "\n",
    "    else:\n",
    "        raise 'Invalid input : please for features_augmentation' \n",
    "                                                 \n",
    "\n",
    "    return network_features_per_convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dict(  network_learning_rate = 0.0005,\n",
    "                    network_n_classes = 3,\n",
    "                    dropout = 0.75,\n",
    "                    network_depth = 6,\n",
    "                    convolution_per_layer = 3,\n",
    "                    network_size_of_convolutions = 3,  \n",
    "                    features_augmentation = 'p10',\n",
    "                    network_first_num_features = 16,\n",
    "                    trainingset = 'simulation_png',\n",
    "                    downsampling = 'maxpooling',\n",
    "                    thresholds = [0, 0.1, 0.8],\n",
    "                    weighted_cost = False,\n",
    "                    batch_size = 1):\n",
    "\n",
    "    \"\"\" features_augmentation : string : first caracter p for addition or x for multiplication. Rest of the string : imcrementation value. \"\"\"\n",
    "\n",
    "    network_learning_rate = network_learning_rate\n",
    "    network_n_classes = network_n_classes\n",
    "    dropout = dropout\n",
    "    network_depth = network_depth\n",
    "    \n",
    "    trainingset = trainingset\n",
    "    downsampling = downsampling\n",
    "    thresholds = thresholds\n",
    "    weighted_cost = weighted_cost\n",
    "\n",
    "    network_first_num_features = network_first_num_features\n",
    "    network_convolution_per_layer = [convolution_per_layer for i in range(network_depth)]\n",
    "    network_size_of_convolutions_per_layer = [[network_size_of_convolutions for k in range(network_convolution_per_layer[i])] for i in range(network_depth)]\n",
    "\n",
    "    network_features_per_convolution = generate_features(network_depth,network_first_num_features,features_augmentation,network_convolution_per_layer)\n",
    "\n",
    "    config = {\n",
    "        'network_learning_rate': network_learning_rate,\n",
    "        'network_n_classes': network_n_classes,\n",
    "        'network_dropout': dropout,\n",
    "        'network_depth': network_depth,\n",
    "        'network_convolution_per_layer': network_convolution_per_layer,\n",
    "        'network_size_of_convolutions_per_layer': network_size_of_convolutions_per_layer,\n",
    "        'network_features_per_convolution': network_features_per_convolution,\n",
    "        'network_trainingset': trainingset,\n",
    "        'network_downsampling': downsampling,\n",
    "        'network_thresholds': thresholds,\n",
    "        'network_weighted_cost': weighted_cost,\n",
    "        'network_batch_size': batch_size\n",
    "    }\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_heliosjob(bashname,configfile,config,path_trainingset,path_model,path_model_init = None):\n",
    "    \"\"\"Generate config file given a config dict. Generate the corresponding submission.\"\"\"\n",
    "\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "\n",
    "    with open(os.path.join(path_model, configfile), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "        \n",
    "    name_model = path_model.split('/')[-2]\n",
    "\n",
    "    file = open(os.path.join(path_model, bashname),\"w\")\n",
    "    file.write(\"#!/bin/bash \\n\")\n",
    "    file.write(\"#PBS -N mapping_test \\n\")\n",
    "    file.write(\"#PBS -A rrp-355-aa \\n\")\n",
    "    file.write(\"#PBS -l walltime=43200 \\n\")\n",
    "    file.write(\"#PBS -l nodes=1:gpus=1 \\n\")\n",
    "    file.write(\"cd $SCRATCH/maxime/axondeepseg/models/\" + name_model + \"/ \\n\")\n",
    "    file.write(\"source /home/maxwab/tf11-py27/bin/activate \\n\")\n",
    "    file.write(\"module load compilers/gcc/4.8.5 compilers/java/1.8 apps/buildtools compilers/swig apps/git apps/bazel/0.4.3 \\n\")\n",
    "    file.write(\"module load cuda/7.5 \\n\")\n",
    "    file.write(\"module load libs/cuDNN/5 \\n\")\n",
    "    file.write(\"python ../../AxonDeepSeg/trainingforhelios.py -co \")\n",
    "    file.write(str(configfile))\n",
    "    file.write(\" -t \") \n",
    "    file.write(str(path_trainingset))\n",
    "    file.write(\" -m \")\n",
    "    file.write(str(path_model))\n",
    "    if path_model_init:\n",
    "        file.write(\" -i \")\n",
    "        file.write(str(path_model_init))\n",
    "        \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_guilliminjob(bashname,configfile,config,path_trainingset,path_model,path_model_init = None):\n",
    "    \"\"\"Generate config file given a config dict. Generate the corresponding submission.\"\"\"\n",
    "\n",
    "    if not os.path.exists(path_model):\n",
    "        os.makedirs(path_model)\n",
    "\n",
    "    with open(os.path.join(path_model, configfile), 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "        \n",
    "    name_model = path_model.split('/')[-2]\n",
    "\n",
    "    file = open(os.path.join(path_model, bashname),\"w\")\n",
    "    file.write(\"#!/bin/bash \\n\")\n",
    "    file.write(\"#PBS -N \"+ name_model +\" \\n\")\n",
    "    file.write(\"#PBS -A rrp-355-aa \\n\")\n",
    "    file.write(\"#PBS -l walltime=43200 \\n\")\n",
    "    file.write(\"#PBS -l nodes=1:gpus=1 \\n\")\n",
    "    file.write(\"cd /gs/project/rrp-355-aa/maxwab/axondeepseg/models/\" + name_model + \"/ \\n\")\n",
    "    file.write(\"module load foss/2015b Python/2.7.12 \\n\")\n",
    "    file.write(\"source ~/maxwab/tf11-py27/bin/activate \\n\")\n",
    "    file.write(\"module load GCC/5.3.0-2.26 Bazel/0.4.4 CUDA/7.5.18 \\n\")\n",
    "    file.write(\"module load Tensorflow/1.0.0-Python-2.7.12 \\n\")\n",
    "    file.write(\"python ../../AxonDeepSeg/trainingforhelios.py -co \")\n",
    "    file.write(str(configfile))\n",
    "    file.write(\" -t \") \n",
    "    file.write(str(path_trainingset))\n",
    "    file.write(\" -m \")\n",
    "    file.write(str(path_model))\n",
    "    if path_model_init:\n",
    "        file.write(\" -i \")\n",
    "        file.write(str(path_model_init))\n",
    "        \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-GENERATING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the name of the files and models, and environment variables.\n",
    "\n",
    "**IMPORTANT TO CHANGE AT EACH GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server = 'Guillimin' # Choose between Guillimin and Helios\n",
    "\n",
    "# Warning: model names must finish by \"/\"\n",
    "modelnames = ['cv_2c_nw_d4_3_b8_x2-16/', 'cv_2c_nw_d5_3_b8_x2-16/', 'cv_2c_nw_d4_3_b8_x2-32/', 'cv_2c_nw_d5_3_b8_x2-32/', \n",
    "              'cv_2c_nw_d4_3_b16_x2-32/', 'cv_2c_nw_d5_3_b16_x2-32/']\n",
    "\n",
    "path_models = '../models/'\n",
    "path_data = '../../../trainingsets/'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining some test config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config1 = generate_dict( network_learning_rate = 0.0005,\n",
    "\t                     network_n_classes = 2,\n",
    "\t                     dropout = 0.75,\n",
    "\t                     network_depth = 4,\n",
    "\t                     convolution_per_layer = 3,\n",
    "\t                     network_size_of_convolutions = 3,  \n",
    "\t                     features_augmentation = 'x2',\n",
    "\t                     network_first_num_features = 16,\n",
    "\t                     trainingset = 'SEM_2classes_reduced',\n",
    "\t                     downsampling = 'convolution',\n",
    "\t                     thresholds = [0, 0.5],\n",
    "\t                     weighted_cost = False,\n",
    "                         batch_size = 8)\n",
    "\n",
    "config2 = generate_dict( network_learning_rate = 0.0005,\n",
    "\t                     network_n_classes = 2,\n",
    "\t                     dropout = 0.75,\n",
    "\t                     network_depth = 5,\n",
    "\t                     convolution_per_layer = 3,\n",
    "\t                     network_size_of_convolutions = 3,  \n",
    "\t                     features_augmentation = 'x2',\n",
    "\t                     network_first_num_features = 16,\n",
    "\t                     trainingset = 'SEM_2classes_reduced',\n",
    "\t                     downsampling = 'convolution',\n",
    "\t                     thresholds = [0, 0.5],\n",
    "\t                     weighted_cost = False,\n",
    "                         batch_size = 8)\n",
    "\n",
    "config3 = generate_dict( network_learning_rate = 0.0005,\n",
    "\t                     network_n_classes = 2,\n",
    "\t                     dropout = 0.75,\n",
    "\t                     network_depth = 4,\n",
    "\t                     convolution_per_layer = 3,\n",
    "\t                     network_size_of_convolutions = 3,  \n",
    "\t                     features_augmentation = 'x2',\n",
    "\t                     network_first_num_features = 32,\n",
    "\t                     trainingset = 'SEM_2classes_reduced',\n",
    "\t                     downsampling = 'convolution',\n",
    "\t                     thresholds = [0, 0.5],\n",
    "\t                     weighted_cost = False,\n",
    "                         batch_size = 8)\n",
    "\n",
    "config4 = generate_dict( network_learning_rate = 0.0005,\n",
    "\t                     network_n_classes = 2,\n",
    "\t                     dropout = 0.75,\n",
    "\t                     network_depth = 5,\n",
    "\t                     convolution_per_layer = 3,\n",
    "\t                     network_size_of_convolutions = 3,  \n",
    "\t                     features_augmentation = 'x2',\n",
    "\t                     network_first_num_features = 32,\n",
    "\t                     trainingset = 'SEM_2classes_reduced',\n",
    "\t                     downsampling = 'convolution',\n",
    "\t                     thresholds = [0, 0.5],\n",
    "\t                     weighted_cost = False,\n",
    "                         batch_size = 8)\n",
    "\n",
    "config5 = generate_dict( network_learning_rate = 0.0005,\n",
    "\t                     network_n_classes = 2,\n",
    "\t                     dropout = 0.75,\n",
    "\t                     network_depth = 4,\n",
    "\t                     convolution_per_layer = 3,\n",
    "\t                     network_size_of_convolutions = 3,  \n",
    "\t                     features_augmentation = 'x2',\n",
    "\t                     network_first_num_features = 32,\n",
    "\t                     trainingset = 'SEM_2classes_reduced',\n",
    "\t                     downsampling = 'convolution',\n",
    "\t                     thresholds = [0, 0.5],\n",
    "\t                     weighted_cost = False,\n",
    "                         batch_size = 16)\n",
    "\n",
    "config6 = generate_dict( network_learning_rate = 0.0005,\n",
    "\t                     network_n_classes = 2,\n",
    "\t                     dropout = 0.75,\n",
    "\t                     network_depth = 5,\n",
    "\t                     convolution_per_layer = 3,\n",
    "\t                     network_size_of_convolutions = 3,  \n",
    "\t                     features_augmentation = 'x2',\n",
    "\t                     network_first_num_features = 32,\n",
    "\t                     trainingset = 'SEM_2classes_reduced',\n",
    "\t                     downsampling = 'convolution',\n",
    "\t                     thresholds = [0, 0.5],\n",
    "\t                     weighted_cost = False,\n",
    "                         batch_size = 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the bash file that will launch all submissions at once, as well as the required sh files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_list = [config1,config2,config3,config4,config5,config6]\n",
    "file = open(path_models + \"/global_submission.sh\",\"w\") # We create the general submission file at the root of the models folder\n",
    "\n",
    "for i,config in enumerate(config_list):\n",
    "\n",
    "    submission_filename = \"submission\"+str(i)+\".sh\"\n",
    "    if server == 'Helios':\n",
    "        file.write('msub ')\n",
    "    elif server == 'Guillimin':\n",
    "        file.write('qsub ')\n",
    "    file.write(str(modelnames[i])+'/')\n",
    "    file.write(str(submission_filename))\n",
    "    if server == 'Guillimin':\n",
    "        file.write(' -l nodes=1:gpus=1:exclusive_process') # To avoid a server error. Check that the number of nodes is the good one !\n",
    "    file.write('\\n')\n",
    "    \n",
    "    if server == 'Helios':\n",
    "        generate_heliosjob(submission_filename, 'config_network.json', config,\n",
    "                       path_trainingset = path_data+config['network_trainingset']+'/training/',\n",
    "                       path_model = path_models+modelnames[i]\n",
    "                      )\n",
    "    elif server == 'Guillimin':\n",
    "        generate_guilliminjob(submission_filename, 'config_network.json', config,\n",
    "                       path_trainingset = path_data+config['network_trainingset']+'/training/',\n",
    "                       path_model = path_models+modelnames[i]\n",
    "                      )\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
