{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import json, pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rescale\n",
    "import time\n",
    "\n",
    "from AxonDeepSeg.apply_model import axon_segmentation\n",
    "from AxonDeepSeg.testing.segmentation_scoring import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to apply each model to the test images to compute the statistics we are going to use to compare the models. \n",
    "It reads an existing json model_comparison file in order not to have to relaunch each model each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most important part: define where to find the models to test as well as the data to test on\n",
    "path_models = '../models/'\n",
    "path_testing = '../data/baseline_validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reminder: the more resolution we have the less rescaling we have to do (thus the smaller the gps is!)\n",
    "\n",
    "gps_dict = {\n",
    "    'SEM_3c_256':0.1,\n",
    "    'SEM_3c_512':0.1, #Because not same semantics as SEM_3c_256 here\n",
    "    'TEM_3c_256':0.02,\n",
    "    'TEM_3c_512':0.01,\n",
    "    'TEM_3c_1024':0.005,\n",
    "    'TEM_3c_reduced':0.02,\n",
    "    'SEM_3c_reduced':0.2,\n",
    "    \n",
    "}\n",
    "\n",
    "stats = {\n",
    "    'best_acc_model':'best_acc_stats',\n",
    "    'best_loss_model':'best_loss_stats',\n",
    "    'model':'evolution_stats'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parameters for the segmentation, like the smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_value = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful function to transform a png mask to a mask with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labellize(mask_raw, thresh = [0, 0.2, 0.8]):\n",
    "    max_ = np.max(mask_raw)\n",
    "    n_c = len(thresh)\n",
    "    \n",
    "    mask = np.zeros_like(mask_raw)\n",
    "    for i, e in enumerate(thresh[1:]):\n",
    "        mask[np.where(mask_raw >= e*255)] = i+1\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(mask_raw):\n",
    "    vals = np.unique(mask_raw)\n",
    "    mask = np.zeros((mask_raw.shape[0], mask_raw.shape[1], len(vals)))\n",
    "    for i,e in enumerate(vals):\n",
    "        mask[:,:,i] = mask_raw == e\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def volumize(mask_labellized, n_class):\n",
    "    '''\n",
    "    :param mask_labellized: 2-D array with each class being indicated as its corresponding \n",
    "    number. ex : [[0,0,1],[2,2,0],[0,1,2]].\n",
    "    '''\n",
    "    mask = np.zeros((mask_labellized.shape[0], mask_labellized.shape[1], n_class))\n",
    "\n",
    "    for i in range(n_class):\n",
    "        mask[:,:,i] = mask_labellized == i\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't want to create an image, but just return the predictions (not the probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for folder in tqdm(os.listdir(path_models),desc='models'): # Loop over all models\n",
    "    # First we load every information independent of the model\n",
    "    # We generate the list of testing folders, each one containing one image\n",
    "    testing_folders = [d for d in os.listdir(path_testing) if os.path.isdir(os.path.join(path_testing, d))]\n",
    "    path_testing_folders = map(lambda x: os.path.join(path_testing,x), testing_folders)\n",
    "    \n",
    "    path_model = os.path.join(path_models, folder)\n",
    "    if os.path.isdir(path_model) and folder[-11:] != 'checkpoints':        \n",
    "        if not os.path.exists(os.path.join(path_model,'model_statistics_validation.json')):\n",
    "            model_comparison_list = []\n",
    "\n",
    "            # First we have to retrieve some essential data from the config file\n",
    "            with open(os.path.join(path_model,'config_network.json'), 'r') as fd:\n",
    "                config_network = json.loads(fd.read())\n",
    "            trainingset_name = config_network['network_trainingset']\n",
    "            type_trainingset = trainingset_name.split('_')[0]\n",
    "            n_classes = config_network['network_n_classes']\n",
    "\n",
    "            # Now we load every information that is independent of the checkpoint\n",
    "            # We retrieve the type of each image\n",
    "            L_names_gps = [type_image + '_3c_' + str(trainingset_name.split('_')[-1]) for type_image in map(lambda x: x.split('_')[0], testing_folders)]\n",
    "            L_gps = [gps_dict[name_gps] for name_gps in L_names_gps]\n",
    "\n",
    "            print 'Beginning segmentation of test images with ' + type_trainingset + ' model, id ' + str(folder) + '...'\n",
    "\n",
    "            # We are now going to loop over all checkpoint files.\n",
    "            for checkpoint in os.listdir(path_model):\n",
    "                if checkpoint[-10:] == '.ckpt.meta':\n",
    "                    result_model = {}\n",
    "                    name_checkpoint = checkpoint[:-10]\n",
    "                    result_model.update({'id_model':folder,\n",
    "                                        'ckpt':name_checkpoint,\n",
    "                                        'config':config_network})\n",
    "\n",
    "                    # First we compute the training statistics, which are independent of the testing images\n",
    "\n",
    "                    # >> Validation 10-moving average statistics\n",
    "                    try:                        \n",
    "                        f = open(path_model + '/'+ stats['name_checkpoint'] + '.pkl', 'r')\n",
    "                        res = pickle.load(f)\n",
    "                        acc_stats = res['accuracy']\n",
    "                        loss_stats = res['loss']\n",
    "                        epoch_stats = res['steps']\n",
    "                    except:\n",
    "                        print 'No stats file found...'\n",
    "                        f = open(path_model + '/evolution.pkl', 'r')\n",
    "                        res = pickle.load(f)\n",
    "                        epoch_stats = max(res['steps'])\n",
    "                        acc_stats = np.mean(res['accuracy'][-10:])\n",
    "                        loss_stats = np.mean(res['loss'][-10:])\n",
    "\n",
    "                    result_model.update({'training_stats':{\n",
    "                        'training_epoch':epoch_stats,\n",
    "                        'training_mvg_avg10_acc':acc_stats,\n",
    "                        'training_mvg_avg10_loss':loss_stats\n",
    "                                                           },\n",
    "                                         'testing_stats':[]\n",
    "                                        })\n",
    "\n",
    "\n",
    "                    # We now want to test on each SEM and TEM image\n",
    "                    # We use each folder of the baseline_testing in data. They have a prefix depending if SEM or TEM\n",
    "                    # Inference time                                \n",
    "                    predictions, pred_probas = axon_segmentation(path_testing_folders, \n",
    "                                                           path_model, \n",
    "                                                           config_network,\n",
    "                                                           ckpt_name = name_checkpoint,\n",
    "                                                           crop_value=crop_value, \n",
    "                                                           general_pixel_sizes=L_gps,\n",
    "                                                           pred_proba=True,\n",
    "                                                           write_mode=False,\n",
    "                                                           gpu_per = 0.3\n",
    "                                                          )\n",
    "\n",
    "                    # We now have a list of predictions and prediction_probas\n",
    "                    print 'Statistics extraction...'\n",
    "                    for i, testing_folder in tqdm(enumerate(testing_folders)):\n",
    "\n",
    "                        # First we get the appropriate prediction and pred proba\n",
    "                        prediction = predictions[i]\n",
    "                        pred_proba = pred_probas[i]\n",
    "\n",
    "                        path_testing_folder = os.path.join(path_testing, testing_folder)\n",
    "\n",
    "                        # Reading the images and processing them if needed\n",
    "                        mask_raw = imread(os.path.join(path_testing_folder, 'mask.png'), flatten=True, mode='L')\n",
    "                        #img_raw = imread(os.path.join(path_testing_folder, 'image.png'), flatten=True, mode='L')\n",
    "                        mask = labellize(mask_raw)\n",
    "\n",
    "                        # We infer the name of the different files\n",
    "                        type_image = testing_folder.split('_')[0] # SEM or TEM\n",
    "                        name_image = '_'.join(testing_folder.split('_')[1:]) # Rest of the name of the image\n",
    "                        testing_stats_dict = {'type_image':type_image, \n",
    "                                              'name_image':name_image}\n",
    "\n",
    "\n",
    "                        '''\n",
    "                        print 'GPS chosen: ' + str(gps)\n",
    "                        file = open(path_testing_folder + '/pixel_size_in_micrometer.txt', 'r')\n",
    "                        pixel_size = float(file.read())\n",
    "                        plt.figure()\n",
    "                        plt.imshow(rescale(img_raw, float(pixel_size)/gps, preserve_range=True)[0:256,0:256], cmap='gray')\n",
    "                        plt.colorbar()\n",
    "                        plt.show();\n",
    "                        '''\n",
    "                        # Processing pred_proba into statistics\n",
    "                        a = np.exp(pred_proba)\n",
    "                        b = np.sum(a, axis=-1)\n",
    "                        pred_proba = np.stack([np.divide(a[:,:,l],b) for l in range(n_classes)], axis=-1)\n",
    "\n",
    "                        # Computation of metrics\n",
    "                        vec_prediction = np.reshape(volumize(prediction,n_classes), (-1,n_classes))\n",
    "                        vec_pred_proba = np.reshape(pred_proba, (-1,n_classes))\n",
    "                        vec_mask = np.reshape(volumize(mask,n_classes), (-1,n_classes))\n",
    "                        # >> Accuracy and XEntropy loss\n",
    "                        testing_stats_dict.update({\n",
    "                            'accuracy':accuracy_score(mask.ravel(), prediction.ravel()),\n",
    "                            'log_loss':log_loss(vec_mask, vec_pred_proba)\n",
    "                                                   })\n",
    "                        # >> Pixel wise dice, both classes, and element wise dice\n",
    "                        gt_axon = volumize(mask,n_classes)[:,:,-1]\n",
    "                        pred_axon = volumize(prediction,n_classes)[:,:,-1]\n",
    "                        pw_dice_axon = pw_dice(pred_axon, gt_axon)\n",
    "                        testing_stats_dict.update({\n",
    "                            'pw_dice_axon':pw_dice_axon})\n",
    "\n",
    "                        '''\n",
    "                        ew_sensitivity_axon, ew_precision_axon, ew_diffusion_axon = score_analysis(img_raw, \n",
    "                                                                                          gt_axon, \n",
    "                                                                                          pred_axon)\n",
    "\n",
    "                        data_axon_dice = dice(img_raw, gt_axon, pred_axon, min_area=4)\n",
    "                        ew_dice_mean_axon = data_axon_dice['dice'].mean()\n",
    "                        ew_dice_quant_axon = data_axon_dice['dice'].quantile([0.1, 0.5, 0.9, 0.95])\n",
    "\n",
    "                        result_model.update({name_testing_image:{\n",
    "                            'ew_dice_mean_axon':ew_dice_mean_axon,\n",
    "                            'ew_dice_quant10_axon':ew_dice_quant_axon.values[0],\n",
    "                            'ew_dice_quant50_axon':ew_dice_quant_axon.values[1],\n",
    "                            'ew_dice_quant90_axon':ew_dice_quant_axon.values[2],\n",
    "                            'ew_dice_quant95_axon':ew_dice_quant_axon.values[3],\n",
    "                            'ew_sentivity_axon':ew_sensitivity_axon,\n",
    "                            'ew_precision_axon':ew_precision_axon,\n",
    "                            'ew_diffusion_axon':ew_diffusion_axon\n",
    "                                                              }\n",
    "                                             })\n",
    "                        '''\n",
    "                        if n_classes == 3:\n",
    "                            gt_myelin = volumize(mask,n_classes)[:,:,1]\n",
    "                            pred_myelin = volumize(prediction,n_classes)[:,:,1]\n",
    "                            pw_dice_myelin = pw_dice(pred_myelin, gt_myelin)\n",
    "\n",
    "                            testing_stats_dict.update({\n",
    "                                'pw_dice_myelin':pw_dice_myelin})\n",
    "\n",
    "                        result_model['testing_stats'].append(testing_stats_dict)\n",
    "\n",
    "                    model_comparison_list.append(result_model)               \n",
    "\n",
    "            # Finally we save the model in a new json file.\n",
    "            path_file = os.path.join(path_model, 'model_statistics_validation.json')\n",
    "            if os.path.exists(path_file):\n",
    "                shutil.move(path_file, os.path.join(path_model, 'model_statistics_validation.json.old'))\n",
    "\n",
    "            existing_dict = {'data':model_comparison_list,\n",
    "                            'date':time.strftime(\"%Y-%m-%d\")}\n",
    "\n",
    "            with open(path_file, 'w') as f:\n",
    "                json.dump(existing_dict, f, indent=2)\n",
    "            model_comparison_list = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the json file into an Excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, os, json\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class metrics():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.path_models = set()\n",
    "        self.stats = pd.DataFrame()\n",
    "        self.filtered_stats = pd.DataFrame()\n",
    "        self.aggregated_stats = pd.DataFrame()\n",
    "        self.columns = ['id_model', 'ckpt', 'type_model','training_acc', \n",
    "                               'training_loss', 'training_epoch', 'pw_dice_myelin', \n",
    "                               'pw_dice_axon', 'testing_log_loss', 'testing_accuracy', 'testing_name_image', \n",
    "                               'testing_type_image']\n",
    "        \n",
    "    def add_models(self,path_models):\n",
    "        if type(path_models) != list:\n",
    "            path_models = [path_models]\n",
    "        [self.path_models.add(e) for e in path_models]\n",
    "            \n",
    "    def load_models(self):\n",
    "        for path in self.path_models:\n",
    "            try:\n",
    "                with open(os.path.join(path, 'model_statistics_validation.json')) as f:\n",
    "                    stats_dict = json.loads(f.read())['data']\n",
    "            except:\n",
    "                print 'No config file found'\n",
    "                \n",
    "            # Now we add a line to the stats dataframe for each model\n",
    "            for ckpt in stats_dict:\n",
    "                print \"found\"\n",
    "                \n",
    "                # Getting each part of data\n",
    "                model_name = ckpt['id_model']\n",
    "                ckpt_name = ckpt['ckpt']\n",
    "                config = ckpt['config']\n",
    "                training_stats = ckpt['training_stats']\n",
    "                training_acc = training_stats['training_mvg_avg10_acc']\n",
    "                training_loss = training_stats['training_mvg_avg10_loss']\n",
    "                training_epoch = training_stats['training_epoch']\n",
    "                testing_stats_list = ckpt['testing_stats']\n",
    "                for testing_stats in testing_stats_list:\n",
    "                    pw_dice_myelin = testing_stats['pw_dice_myelin']\n",
    "                    pw_dice_axon = testing_stats['pw_dice_axon']\n",
    "                    testing_log_loss = testing_stats['log_loss']\n",
    "                    name_image = testing_stats['name_image']\n",
    "                    type_image = testing_stats['type_image']\n",
    "                    testing_accuracy = testing_stats['accuracy']\n",
    "\n",
    "                    new_line = [[model_name, ckpt_name, config['network_trainingset'].split('_')[0], \n",
    "                                   training_acc, training_loss, training_epoch, pw_dice_myelin, pw_dice_axon,\n",
    "                                  testing_log_loss, testing_accuracy, name_image, type_image]]\n",
    "                    \n",
    "                    # Updating the dataframe with the latest data\n",
    "                    self.stats = self.stats.append(pd.DataFrame(columns=self.columns, data=new_line))\n",
    "                \n",
    "                self.filtered_stats = self.stats.copy()\n",
    "                \n",
    "    \n",
    "    def filter_(self, list_acquisitions = None, list_ckpt = None, write_mode=False, name_file=None):\n",
    "        filtered_stats = pd.DataFrame()\n",
    "        \n",
    "        if list_acquisitions != None:\n",
    "            # Processing arguments\n",
    "            if type(list_acquisitions) != list:\n",
    "                list_acquisitions = [list_acquisitions]\n",
    "\n",
    "            # For each acquisition type\n",
    "            for image_to_take in list_acquisitions:\n",
    "                filtered_stats = filtered_stats.append(self.stats.loc[self.stats['testing_type_image']==image_to_take])\n",
    "        if list_ckpt != None:       \n",
    "            # Processing arguments\n",
    "            if type(list_ckpt) != list:\n",
    "                list_ckpt = [list_ckpt]    \n",
    "            for ckpt in list_ckpt:\n",
    "                filtered_stats = filtered_stats.append(self.stats.loc[self.stats['ckpt']==ckpt])\n",
    "        self.filtered_stats = filtered_stats\n",
    "        \n",
    "        if write_mode == True:\n",
    "            if name_file is None:\n",
    "                name_file = 'filtered_'+'_'.join(list_acquisitions)+'_'+time.strftime(\"%Y-%m-%d\")+'.csv'\n",
    "            filtered_stats.T.to_csv(name_file)\n",
    "            \n",
    "        # Outputting the filtered pandas dataframe.\n",
    "        return filtered_stats\n",
    "    \n",
    "   \n",
    "    def aggregate(self, list_metrics,write_mode=False, name_file = None):\n",
    "        # Processing arguments\n",
    "        aggregated_stats = pd.DataFrame()\n",
    "        if type(list_metrics) != list:\n",
    "            list_metrics = [list_metrics]\n",
    "            \n",
    "        for metric in list_metrics:\n",
    "            tmp = self.filtered_stats.groupby(['id_model', 'ckpt']).apply(metric)\n",
    "            tmp.columns = map(lambda x: x+'_'+metric.__name__, tmp.columns.tolist())\n",
    "            aggregated_stats = pd.concat([aggregated_stats, tmp],\n",
    "                                             axis = 1, ignore_index=False)\n",
    "            \n",
    "        if write_mode == True:\n",
    "            if name_file is None:\n",
    "                name_file = 'agg_'+'_'.join(map(lambda x: x.__name__, list_metrics))+'_'+time.strftime(\"%Y-%m-%d\")+'.csv'\n",
    "            aggregated_stats.T.to_csv(name_file)\n",
    "            \n",
    "        return aggregated_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_list = [os.path.join('../test_models',e) for e in os.listdir('../test_models') if os.path.isdir(os.path.join('../test_models/', e))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met = metrics()\n",
    "met.add_models(model_list)\n",
    "#met.add_models([os.path.join('../models','baseline-SEM256new-9438/')])\n",
    "#met.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.filtered_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.filter_(list_acquisitions=['SEM'], write_mode=False, name_file='onSEM-5185.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.aggregate([np.mean], write_mode=True, name_file='onSEM-5185.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
