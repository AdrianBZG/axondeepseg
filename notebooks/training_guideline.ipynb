{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new model\n",
    "\n",
    "In this notebook we will learn how to train a new model for axon & myelin segmentation. It covers the following scenario:\n",
    "\n",
    "* Train a model from scratch by defining the parameters of the network\n",
    "* Make inference using the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from scipy.misc import imread, imsave\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from shutil import copy\n",
    "\n",
    "from AxonDeepSeg.train_network import train_model\n",
    "from AxonDeepSeg.apply_model import axon_segmentation\n",
    "\n",
    "# reset the tensorflow graph for new training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a new model\n",
    "#### 1.1. Create subfolder to save new model and its parameters.\n",
    "\n",
    "For simplicity, the new model will be created under the `models/` folder in the AxonDeepSeg repository. The name of the model folder will be generated automatically using the date and time (to avoid multiple instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to where the trained model will be saved\n",
    "dir_name = time.strftime(\"%Y-%m-%d\") + '_' + time.strftime(\"%H-%M-%S\")\n",
    "path_model = os.path.join('../models/', dir_name)\n",
    "\n",
    "# Create directory\n",
    "if not os.path.exists(path_model):\n",
    "    os.makedirs(path_model)\n",
    "\n",
    "file_config = 'config_network.json'  # file name of network configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Define the name and path of the training set\n",
    "\n",
    "Here we assume that the training data folder has already been created by following the guidelines detailed in [guide_dataset_building.ipynb](https://github.com/neuropoly/axondeepseg/blob/master/notebooks/guide_dataset_building.ipynb).\n",
    "\n",
    "The expected structure of the training data folder is the following:\n",
    "\n",
    "~~~\n",
    "data_training\n",
    " └── Train\n",
    "      └── image_0.png\n",
    "      └── mask_0.png\n",
    "      └── image_1.png\n",
    "      └── mask_1.png\n",
    "          ...\n",
    " └── Validation\n",
    "      └── image_0.png\n",
    "      └── mask_0.png\n",
    "      └── image_1.png\n",
    "      └── mask_1.png\n",
    "          ...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingset_name = 'my_awesome_model'\n",
    "trainingset_name = 'SEM_3c_512'\n",
    "# path_training = '/home/jondoe/data_training'  #  folder containing training data\n",
    "path_training = '/home/neuropoly/mikula/data/patched_data'  #  folder containing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Define the configuration parameters of the training\n",
    "\n",
    "The networks and training parameters (i.e. hyperparameters) used in the original [AxonDeepSeg article](https://www.nature.com/articles/s41598-018-22181-4) are defined below for TEM and SEM. **Note that these architectures might not produce satisfactory results on your data.**\n",
    "\n",
    "Importantly: the pixel size is not defined at the training step. During inference however, the parameter `-t {SEM,TEM}` sets the resampling resolution to 0.1µm or 0.01µm respectively (i.e., implying the pixel size of the training data is 0.1µm or 0.01µm respectively). This is definitely a limitation of the current version of AxonDeepSeg, which we are planning to solve at some point (for more info, see [Issue #152](https://github.com/neuropoly/axondeepseg/issues/152)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of network configuration for TEM data\n",
    "config = {\n",
    "    \n",
    "# General parameters:    \n",
    "  \"n_classes\": 3,  # Number of classes. For this application, the number of classes should be set to **3** (i.e. axon pixel, myelin pixel, or background pixel).\n",
    "  \"thresholds\": [0, 0.2, 0.8],  # Thresholds for the 3-class classification problem. Do not modify.  \n",
    "  \"trainingset_patchsize\": 512,  # Patch size of the training set in pixels (note that the patches have the same size in both dimensions).  \n",
    "  \"trainingset\": trainingset_name,  # Name of the training set.\n",
    "  \"batch_size\": 8,  # Batch size, i.e. the number of training patches used in one iteration of the training. Note that a larger batch size will take more memory.\n",
    "\n",
    "# Network architecture parameters:     \n",
    "  \"depth\": 4,  # Depth of the network (i.e. number of blocks of the U-net).\n",
    "  \"convolution_per_layer\": [3, 3, 3, 3],  # Number of convolution layers used at each block.\n",
    "  \"size_of_convolutions_per_layer\": [[5, 5, 5], [3, 3, 3], [3, 3, 3], [3, 3, 3]],  # Kernel size of each convolution layer of the network.\n",
    "  \"features_per_convolution\": [[[1, 16], [16, 16], [16, 16]], [[16, 32], [32, 32], [32, 32]], [[32, 64], [64, 64], [64, 64]], [[64, 128], [128, 128], [128, 128]]],  # Number of features of each convolution layer.\n",
    "  \"downsampling\": \"convolution\",  # Type of downsampling to use in the downsampling layers of the network. Option \"maxpooling\" for standard max pooling layer or option \"convolution\" for learned convolutional downsampling.\n",
    "  \"dropout\": 0.75,  # Dropout to use for the training. Note: In TensorFlow, the keep probability is used instead. For instance, setting this param. to 0.75 means that 75% of the neurons of the network will be kept (i.e. dropout of 25%).\n",
    "     \n",
    "# Learning rate parameters:    \n",
    "  \"learning_rate\": 0.001,  # Learning rate to use in the training.  \n",
    "  \"learning_rate_decay_activate\": True,  # Set to \"True\" to use a decay on the learning rate.  \n",
    "  \"learning_rate_decay_period\": 24000,  # Period of the learning rate decay, expressed in number of images (samples) seen.\n",
    "  \"learning_rate_decay_type\": \"polynomial\",  # Type of decay to use. An exponential decay will be used by default unless this param. is set to \"polynomial\" (to use a polynomial decay).\n",
    "  \"learning_rate_decay_rate\": 0.99,  # Rate of the decay to use for the exponential decay. This only applies when the user does not set the decay type to \"polynomial\".\n",
    "    \n",
    "# Batch normalization parameters:     \n",
    "  \"batch_norm_activate\": True,  # Set to \"True\" to use batch normalization during the training.\n",
    "  \"batch_norm_decay_decay_activate\": True,  # Set to \"True\" to activate an exponential decay for the batch normalization step of the training.  \n",
    "  \"batch_norm_decay_starting_decay\": 0.7,  # The starting decay value for the batch normalization. \n",
    "  \"batch_norm_decay_ending_decay\": 0.9,  # The ending decay value for the batch normalization.\n",
    "  \"batch_norm_decay_decay_period\": 16000,  # Period of the batch normalization decay, expressed in number of images (samples) seen.\n",
    "        \n",
    "# Weighted cost parameters:    \n",
    "  \"weighted_cost-activate\": True,  # Set to \"True\" to use weights based on the class in the cost function for the training.\n",
    "  \"weighted_cost-balanced_activate\": True,  # Set to \"True\" to use weights in the cost function to correct class imbalance. \n",
    "  \"weighted_cost-balanced_weights\": [1.1, 1, 1.3],  # Values of the weights for the class imbalance. Typically, larger weights are assigned to classes with less pixels to add more penalty in the cost function when there is a misclassification. Order of the classes in the weights list: background, myelin, axon.\n",
    "  \"weighted_cost-boundaries_sigma\": 2,  # Set to \"True\" to add weights to the boundaries (e.g. penalize more when misclassification happens in the axon-myelin interface).\n",
    "  \"weighted_cost-boundaries_activate\": False,  # Value to control the distribution of the boundary weights (if activated). \n",
    "    \n",
    "# Data augmentation parameters:\n",
    "# For each type of data augmentation, the order needs to be specified if you decide to apply more than one \n",
    "# transformation sequentially. For instance, setting the \"da-0-shifting-activate\" parameter to 'True' means that the \n",
    "# shifting is the first transformation that will be applied to the sample(s) during training. The default ranges of \n",
    "# transformations are:\n",
    "#   Shifing: Random horizontal and vertical shifting between 0 and 10% of the patch size, sampled from a uniform distribution.\n",
    "#   Rotation: Random rotation, angle between 5 and 89 degrees, sampled from a uniform distribution.\n",
    "#   Rescaling: Random rescaling of a randomly sampled factor between 1/1.2 and 1.2.\n",
    "#   Flipping: Random fipping: vertical fipping or horizontal fipping.\n",
    "#   Blurring: Gaussian blur with the standard deviation of the gaussian kernel being uniformly sampled between 0 and 4.\n",
    "#   Elastic deformation: Random elastic deformation with uniformly sampled deformation coefficient α=[1–8] and fixed standard deviation σ=4.\n",
    "# You can find more information about the range of transformations applied to the patches for each data augmentation technique in the file \"data_augmentation.py\".\n",
    "  \"da-type\": \"all\",  # Type of data augmentation procedure. Option **\"all\"** applies all selected data augmentation transformations sequentially, while option **\"random\"** only applies one of the selected transformations (randomly) to the sample(s). Type of data augmentation procedure. Option \"all\" applies all selected data augmentation transformations sequentially, while option \"random\" only applies one of the selected transformations (randomly) to the sample(s). List of available data augmentation transformations: 'random_rotation', 'noise_addition', 'elastic', 'shifting', 'rescaling' and 'flipping'. \n",
    "  \"da-2-random_rotation-activate\": False,  \n",
    "  \"da-5-noise_addition-activate\": False, \n",
    "  \"da-3-elastic-activate\": True, \n",
    "  \"da-0-shifting-activate\": True, \n",
    "  \"da-4-flipping-activate\": True, \n",
    "  \"da-1-rescaling-activate\": False    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of network configuration for SEM data\n",
    "config = {\n",
    "    \n",
    "# General parameters:    \n",
    "  \"n_classes\": 3,  # Number of classes. For this application, the number of classes should be set to **3** (i.e. axon pixel, myelin pixel, or background pixel).\n",
    "  \"thresholds\": [0, 0.2, 0.8],  # Thresholds for the 3-class classification problem. Do not modify.  \n",
    "  \"trainingset_patchsize\": 512,  # Patch size of the training set in pixels (note that the patches have the same size in both dimensions).  \n",
    "  \"trainingset\": trainingset_name,  # Name of the training set.\n",
    "  \"batch_size\": 8,  # Batch size, i.e. the number of training patches used in one iteration of the training. Note that a larger batch size will take more memory.\n",
    "\n",
    "# Network architecture parameters:     \n",
    "  \"depth\": 4,  # Depth of the network (i.e. number of blocks of the U-net).\n",
    "  \"convolution_per_layer\": [2, 2],  # Number of convolution layers used at each block.\n",
    "  \"size_of_convolutions_per_layer\": [[3, 3], [3, 3]],  # Kernel size of each convolution layer of the network.\n",
    "  \"features_per_convolution\": [[[1, 5], [5, 5]], [[5, 10], [10, 10]]],  # Number of features of each convolution layer.\n",
    "  \"downsampling\": \"maxpooling\",  # Type of downsampling to use in the downsampling layers of the network. Option \"maxpooling\" for standard max pooling layer or option \"convolution\" for learned convolutional downsampling.\n",
    "  \"dropout\": 0.75,  # Dropout to use for the training. Note: In TensorFlow, the keep probability is used instead. For instance, setting this param. to 0.75 means that 75% of the neurons of the network will be kept (i.e. dropout of 25%).\n",
    "     \n",
    "# Learning rate parameters:    \n",
    "  \"learning_rate\": 0.001,  # Learning rate to use in the training.  \n",
    "  \"learning_rate_decay_activate\": True,  # Set to \"True\" to use a decay on the learning rate.  \n",
    "  \"learning_rate_decay_period\": 24000,  # Period of the learning rate decay, expressed in number of images (samples) seen.\n",
    "  \"learning_rate_decay_type\": \"polynomial\",  # Type of decay to use. An exponential decay will be used by default unless this param. is set to \"polynomial\" (to use a polynomial decay).\n",
    "  \"learning_rate_decay_rate\": 0.99,  # Rate of the decay to use for the exponential decay. This only applies when the user does not set the decay type to \"polynomial\".\n",
    "    \n",
    "# Batch normalization parameters:     \n",
    "  \"batch_norm_activate\": True,  # Set to \"True\" to use batch normalization during the training.\n",
    "  \"batch_norm_decay_decay_activate\": True,  # Set to \"True\" to activate an exponential decay for the batch normalization step of the training.  \n",
    "  \"batch_norm_decay_starting_decay\": 0.7,  # The starting decay value for the batch normalization. \n",
    "  \"batch_norm_decay_ending_decay\": 0.9,  # The ending decay value for the batch normalization.\n",
    "  \"batch_norm_decay_decay_period\": 16000,  # Period of the batch normalization decay, expressed in number of images (samples) seen.\n",
    "        \n",
    "# Weighted cost parameters:    \n",
    "  \"weighted_cost-activate\": True,  # Set to \"True\" to use weights based on the class in the cost function for the training.\n",
    "  \"weighted_cost-balanced_activate\": True,  # Set to \"True\" to use weights in the cost function to correct class imbalance. \n",
    "  \"weighted_cost-balanced_weights\": [1.1, 1, 1.3],  # Values of the weights for the class imbalance. Typically, larger weights are assigned to classes with less pixels to add more penalty in the cost function when there is a misclassification. Order of the classes in the weights list: background, myelin, axon.\n",
    "  \"weighted_cost-boundaries_sigma\": 2,  # Set to \"True\" to add weights to the boundaries (e.g. penalize more when misclassification happens in the axon-myelin interface).\n",
    "  \"weighted_cost-boundaries_activate\": False,  # Value to control the distribution of the boundary weights (if activated). \n",
    "    \n",
    "# Data augmentation parameters:\n",
    "# For each type of data augmentation, the order needs to be specified if you decide to apply more than one \n",
    "# transformation sequentially. For instance, setting the \"da-0-shifting-activate\" parameter to 'True' means that the \n",
    "# shifting is the first transformation that will be applied to the sample(s) during training. The default ranges of \n",
    "# transformations are:\n",
    "#   Shifing: Random horizontal and vertical shifting between 0 and 10% of the patch size, sampled from a uniform distribution.\n",
    "#   Rotation: Random rotation, angle between 5 and 89 degrees, sampled from a uniform distribution.\n",
    "#   Rescaling: Random rescaling of a randomly sampled factor between 1/1.2 and 1.2.\n",
    "#   Flipping: Random fipping: vertical fipping or horizontal fipping.\n",
    "#   Blurring: Gaussian blur with the standard deviation of the gaussian kernel being uniformly sampled between 0 and 4.\n",
    "#   Elastic deformation: Random elastic deformation with uniformly sampled deformation coefficient α=[1–8] and fixed standard deviation σ=4.\n",
    "# You can find more information about the range of transformations applied to the patches for each data augmentation technique in the file \"data_augmentation.py\".\n",
    "  \"da-type\": \"all\",  # Type of data augmentation procedure. Option **\"all\"** applies all selected data augmentation transformations sequentially, while option **\"random\"** only applies one of the selected transformations (randomly) to the sample(s). Type of data augmentation procedure. Option \"all\" applies all selected data augmentation transformations sequentially, while option \"random\" only applies one of the selected transformations (randomly) to the sample(s). List of available data augmentation transformations: 'random_rotation', 'noise_addition', 'elastic', 'shifting', 'rescaling' and 'flipping'. \n",
    "  \"da-2-random_rotation-activate\": False,  \n",
    "  \"da-5-noise_addition-activate\": False, \n",
    "  \"da-3-elastic-activate\": True, \n",
    "  \"da-0-shifting-activate\": True, \n",
    "  \"da-4-flipping-activate\": True, \n",
    "  \"da-1-rescaling-activate\": False    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Save configuration parameters of the network as configuration file (.json)\n",
    "\n",
    "After the config. parameters of the network to be trained are defined, they are saved into a .json file in the model folder. This .json file keeps tract of the network and model parameters in a structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if config file already exists\n",
    "fname_config = os.path.join(path_model+file_config)\n",
    "if os.path.exists(fname_config):\n",
    "    with open(fname_config, 'r') as fd:\n",
    "        config_network = json.loads(fd.read())\n",
    "else:\n",
    "    with open(fname_config, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    with open(fname_config, 'r') as fd:\n",
    "        config_network = json.loads(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Launch the training procedure\n",
    "\n",
    "The training can be launched by calling the *'train_model'* function. After each epoch, the function will display the loss and accuracy of the model. The model is automatically saved at every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Layer: ', 0, ' Conv: ', 0, 'Features: ', [1, 16])\n",
      "('Size:', 5)\n",
      "('Layer: ', 0, ' Conv: ', 1, 'Features: ', [16, 16])\n",
      "('Size:', 5)\n",
      "('Layer: ', 1, ' Conv: ', 0, 'Features: ', [16, 32])\n",
      "('Size:', 3)\n",
      "('Layer: ', 1, ' Conv: ', 1, 'Features: ', [32, 32])\n",
      "('Size:', 3)\n",
      "('Layer: ', 2, ' Conv: ', 0, 'Features: ', [32, 64])\n",
      "('Size:', 3)\n",
      "('Layer: ', 2, ' Conv: ', 1, 'Features: ', [64, 64])\n",
      "('Size:', 3)\n",
      "('Layer: ', 3, ' Conv: ', 0, 'Features: ', [64, 128])\n",
      "('Size:', 3)\n",
      "('Layer: ', 3, ' Conv: ', 1, 'Features: ', [128, 128])\n",
      "('Size:', 3)\n",
      "('Layer: ', 0, ' Conv: ', 0, 'Features: ', [1, 16])\n",
      "('Size:', 5)\n",
      "('Layer: ', 0, ' Conv: ', 1, 'Features: ', [16, 16])\n",
      "('Size:', 5)\n",
      "('Layer: ', 1, ' Conv: ', 0, 'Features: ', [16, 32])\n",
      "('Size:', 3)\n",
      "('Layer: ', 1, ' Conv: ', 1, 'Features: ', [32, 32])\n",
      "('Size:', 3)\n",
      "('Layer: ', 2, ' Conv: ', 0, 'Features: ', [32, 64])\n",
      "('Size:', 3)\n",
      "('Layer: ', 2, ' Conv: ', 1, 'Features: ', [64, 64])\n",
      "('Size:', 3)\n",
      "('Layer: ', 3, ' Conv: ', 0, 'Features: ', [64, 128])\n",
      "('Size:', 3)\n",
      "('Layer: ', 3, ' Conv: ', 1, 'Features: ', [128, 128])\n",
      "('Size:', 3)\n",
      "Total number of parameters to train: 1552387\n",
      "training start\n",
      "Weighted cost selected\n",
      "2018-11-10 21:34:04.324980-epoch:1-loss:13.281106948852539-acc:0.363797664642334\n",
      "2018-11-10 21:34:17.221904-epoch:2-loss:13.03281021118164-acc:0.5117766857147217\n",
      "2018-11-10 21:34:30.060057-epoch:3-loss:5.55285120010376-acc:0.7031128406524658\n",
      "2018-11-10 21:34:42.903981-epoch:4-loss:1.168505609035492-acc:0.7705338001251221\n",
      "2018-11-10 21:34:55.737110-epoch:5-loss:0.8758303821086884-acc:0.8012282848358154\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:35:09.107038-epoch:6-loss:0.6626666784286499-acc:0.835364818572998\n",
      "2018-11-10 21:35:21.945353-epoch:7-loss:0.6799136996269226-acc:0.8377001285552979\n",
      "2018-11-10 21:35:34.840625-epoch:8-loss:1.6246073246002197-acc:0.828465461730957\n",
      "2018-11-10 21:35:47.779338-epoch:9-loss:3.698079824447632-acc:0.8222453594207764\n",
      "2018-11-10 21:36:00.700919-epoch:10-loss:1.3635812401771545-acc:0.8181397914886475\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:36:15.053258-epoch:11-loss:0.6216951608657837-acc:0.8387742042541504\n",
      "2018-11-10 21:36:27.945024-epoch:12-loss:0.6735448241233826-acc:0.8373610973358154\n",
      "2018-11-10 21:36:40.846724-epoch:13-loss:0.6089150607585907-acc:0.8386960029602051\n",
      "2018-11-10 21:36:53.705413-epoch:14-loss:0.6701042354106903-acc:0.8399088382720947\n",
      "2018-11-10 21:37:06.608890-epoch:15-loss:0.8754351437091827-acc:0.8420648574829102\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:37:21.136955-epoch:16-loss:0.9608268439769745-acc:0.8410704135894775\n",
      "2018-11-10 21:37:34.078814-epoch:17-loss:0.6131828129291534-acc:0.8399257659912109\n",
      "2018-11-10 21:37:46.996875-epoch:18-loss:0.6346133947372437-acc:0.8443171977996826\n",
      "2018-11-10 21:37:59.920913-epoch:19-loss:0.5978790819644928-acc:0.8443219661712646\n",
      "2018-11-10 21:38:12.830155-epoch:20-loss:0.5166441202163696-acc:0.8464243412017822\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:38:27.124180-epoch:21-loss:0.5161015689373016-acc:0.8452808856964111\n",
      "2018-11-10 21:38:40.020526-epoch:22-loss:0.5160613358020782-acc:0.8425533771514893\n",
      "2018-11-10 21:38:52.885203-epoch:23-loss:0.5204664170742035-acc:0.8414936065673828\n",
      "2018-11-10 21:39:05.770144-epoch:24-loss:0.506271481513977-acc:0.8438072204589844\n",
      "2018-11-10 21:39:18.619637-epoch:25-loss:0.5191075205802917-acc:0.8447935581207275\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:39:32.964402-epoch:26-loss:0.48755092918872833-acc:0.8491215705871582\n",
      "2018-11-10 21:39:45.891371-epoch:27-loss:0.4997157156467438-acc:0.8471729755401611\n",
      "2018-11-10 21:39:58.790340-epoch:28-loss:0.5063665211200714-acc:0.8461244106292725\n",
      "2018-11-10 21:40:11.758567-epoch:29-loss:0.4704269617795944-acc:0.8565843105316162\n",
      "2018-11-10 21:40:24.615179-epoch:30-loss:0.4764086604118347-acc:0.8591921329498291\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:40:38.967190-epoch:31-loss:0.47949180006980896-acc:0.8603823184967041\n",
      "2018-11-10 21:40:51.911727-epoch:32-loss:0.47652609646320343-acc:0.8598818778991699\n",
      "2018-11-10 21:41:04.800362-epoch:33-loss:0.48445357382297516-acc:0.8570890426635742\n",
      "2018-11-10 21:41:17.685283-epoch:34-loss:0.4607846736907959-acc:0.8679068088531494\n",
      "2018-11-10 21:41:30.554316-epoch:35-loss:0.4898097813129425-acc:0.8560879230499268\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:41:44.881297-epoch:36-loss:0.47694478929042816-acc:0.8609945774078369\n",
      "2018-11-10 21:41:57.752593-epoch:37-loss:0.4623196870088577-acc:0.8684616088867188\n",
      "2018-11-10 21:42:10.650561-epoch:38-loss:0.48255655169487-acc:0.8576064109802246\n",
      "2018-11-10 21:42:23.529657-epoch:39-loss:0.4629281908273697-acc:0.8651714324951172\n",
      "2018-11-10 21:42:36.414285-epoch:40-loss:0.4940441697835922-acc:0.856025218963623\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:42:50.773643-epoch:41-loss:0.46300673484802246-acc:0.8654556274414062\n",
      "2018-11-10 21:43:03.631757-epoch:42-loss:0.48881201446056366-acc:0.8562755584716797\n",
      "2018-11-10 21:43:16.518038-epoch:43-loss:0.47369377315044403-acc:0.8622035980224609\n",
      "2018-11-10 21:43:29.407084-epoch:44-loss:0.46419012546539307-acc:0.8660309314727783\n",
      "2018-11-10 21:43:42.286936-epoch:45-loss:0.44897548854351044-acc:0.8712875843048096\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:43:56.640204-epoch:46-loss:0.48249728977680206-acc:0.8614537715911865\n",
      "2018-11-10 21:44:09.555926-epoch:47-loss:0.4240708202123642-acc:0.8743190765380859\n",
      "2018-11-10 21:44:22.411441-epoch:48-loss:0.455387219786644-acc:0.8708863258361816\n",
      "2018-11-10 21:44:35.283275-epoch:49-loss:0.499318465590477-acc:0.8558392524719238\n",
      "2018-11-10 21:44:48.151312-epoch:50-loss:0.43648575246334076-acc:0.8722012042999268\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:45:02.480111-epoch:51-loss:0.4306401014328003-acc:0.876673698425293\n",
      "2018-11-10 21:45:15.372523-epoch:52-loss:0.4874919354915619-acc:0.862534761428833\n",
      "2018-11-10 21:45:28.194405-epoch:53-loss:0.4665464013814926-acc:0.870542049407959\n",
      "2018-11-10 21:45:41.063667-epoch:54-loss:0.47978349030017853-acc:0.8668308258056641\n",
      "2018-11-10 21:45:53.947626-epoch:55-loss:0.4722634106874466-acc:0.8679261207580566\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:46:08.415406-epoch:56-loss:0.49227455258369446-acc:0.8650674819946289\n",
      "2018-11-10 21:46:21.252562-epoch:57-loss:0.44940564036369324-acc:0.8757236003875732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-10 21:46:34.121853-epoch:58-loss:0.36253155767917633-acc:0.8915863037109375\n",
      "2018-11-10 21:46:46.948825-epoch:59-loss:0.43030229210853577-acc:0.8755509853363037\n",
      "2018-11-10 21:46:59.776982-epoch:60-loss:0.44730304181575775-acc:0.8778009414672852\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:47:14.042641-epoch:61-loss:0.5076057314872742-acc:0.8630046844482422\n",
      "2018-11-10 21:47:26.883356-epoch:62-loss:0.4303636699914932-acc:0.8789882659912109\n",
      "2018-11-10 21:47:39.746626-epoch:63-loss:0.4539366364479065-acc:0.8736264705657959\n",
      "2018-11-10 21:47:52.600319-epoch:64-loss:0.4647601693868637-acc:0.8750269412994385\n",
      "2018-11-10 21:48:05.484794-epoch:65-loss:0.3473314493894577-acc:0.8918039798736572\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:48:19.823789-epoch:66-loss:0.41269350051879883-acc:0.8800323009490967\n",
      "2018-11-10 21:48:32.695211-epoch:67-loss:0.44109101593494415-acc:0.8802978992462158\n",
      "2018-11-10 21:48:45.535888-epoch:68-loss:0.4987519085407257-acc:0.8673141002655029\n",
      "2018-11-10 21:48:58.382351-epoch:69-loss:0.4286377280950546-acc:0.8829610347747803\n",
      "2018-11-10 21:49:11.292014-epoch:70-loss:0.47952935099601746-acc:0.8747475147247314\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:49:24.614066-epoch:71-loss:0.47390539944171906-acc:0.8762035369873047\n",
      "2018-11-10 21:49:37.479996-epoch:72-loss:0.4265294373035431-acc:0.8846807479858398\n",
      "2018-11-10 21:49:50.345312-epoch:73-loss:0.44416069984436035-acc:0.8778343200683594\n",
      "2018-11-10 21:50:03.185965-epoch:74-loss:0.472235843539238-acc:0.8737833499908447\n",
      "2018-11-10 21:50:16.134022-epoch:75-loss:0.44861768186092377-acc:0.8809077739715576\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:50:29.949746-epoch:76-loss:0.49327607452869415-acc:0.8753738403320312\n",
      "2018-11-10 21:50:42.804080-epoch:77-loss:0.4583558738231659-acc:0.8768458366394043\n",
      "2018-11-10 21:50:55.689998-epoch:78-loss:0.42231230437755585-acc:0.8824193477630615\n",
      "2018-11-10 21:51:08.590490-epoch:79-loss:0.43973228335380554-acc:0.8844852447509766\n",
      "2018-11-10 21:51:21.438259-epoch:80-loss:0.4814721792936325-acc:0.877779483795166\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:51:35.251071-epoch:81-loss:0.44765937328338623-acc:0.8820962905883789\n",
      "2018-11-10 21:51:48.101455-epoch:82-loss:0.48229001462459564-acc:0.8788506984710693\n",
      "2018-11-10 21:52:00.935489-epoch:83-loss:0.3081284165382385-acc:0.9002931118011475\n",
      "2018-11-10 21:52:13.741650-epoch:84-loss:0.4125019758939743-acc:0.8835034370422363\n",
      "2018-11-10 21:52:26.632128-epoch:85-loss:0.43255768716335297-acc:0.8813309669494629\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:52:40.901326-epoch:86-loss:0.45577460527420044-acc:0.8814051151275635\n",
      "2018-11-10 21:52:53.728775-epoch:87-loss:0.502881646156311-acc:0.8753886222839355\n",
      "2018-11-10 21:53:06.595425-epoch:88-loss:0.46046046912670135-acc:0.8791427612304688\n",
      "2018-11-10 21:53:19.501646-epoch:89-loss:0.3099979758262634-acc:0.9036147594451904\n",
      "2018-11-10 21:53:32.370951-epoch:90-loss:0.4266577661037445-acc:0.8821091651916504\n",
      "Best accuracy model saved in file: ../models/2018-11-10_21-32-36/best_acc_model.ckpt\n",
      "Best loss model saved in file: ../models/2018-11-10_21-32-36/best_loss_model.ckpt\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:53:46.633406-epoch:91-loss:0.42462827265262604-acc:0.8872106075286865\n",
      "2018-11-10 21:53:59.594072-epoch:92-loss:0.4422803372144699-acc:0.8853964805603027\n",
      "2018-11-10 21:54:12.433665-epoch:93-loss:0.5098327994346619-acc:0.8720753192901611\n",
      "2018-11-10 21:54:25.290486-epoch:94-loss:0.4823029190301895-acc:0.8785796165466309\n",
      "2018-11-10 21:54:38.130091-epoch:95-loss:0.4363261014223099-acc:0.8846840858459473\n",
      "Model saved in file: ../models/2018-11-10_21-32-36/model.ckpt\n",
      "2018-11-10 21:54:51.438032-epoch:96-loss:0.471746027469635-acc:0.8800907135009766\n"
     ]
    }
   ],
   "source": [
    "# reset the tensorflow graph for new testing\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_model(path_training, path_model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. Monitor the training with Tensorboard\n",
    "\n",
    "TensorBoard can be used to monitor the training procedure (loss and accuracy graphs, gradients, activations, identify bugs, etc.). To run TensorBoard, activate ADS virtual environment and run:\n",
    "```\n",
    "tensorboard --logdir PATH_MODEL --port 6006\n",
    "```\n",
    "where `PATH_MODEL` corresponds to this notebook's `path_model` variable (folder where model is being trained), and `port` is the port number where the TensorBoard local web server will be sent to (e.g., port 6006). Once the command is run, open a web browser with the address:\n",
    "```\n",
    "http://localhost:6006/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the trained model\n",
    "#### 2.1. Set the path of the test image to be segmented with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = '/home/neuropoly/mikula/data/ground_truths/443.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Set the path of the test image to be segmented with the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Set the folder name of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imageio.imread(path_img)\n",
    "path_folder, file_name = os.path.split(path_img)\n",
    "\n",
    "path_configfile = os.path.join(path_model,'config_network.json')\n",
    "with open(path_configfile, 'r') as fd:\n",
    "    config_network = json.loads(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Launch the image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REMOVE\n",
    "path_model = \"/home/neuropoly/axondeepseg/models/2018-11-10_18-25-37/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading acquisitions ...\n",
      "Rescaling acquisitions to the target resolution ...\n",
      "Graph construction ...\n",
      "Beginning inference ...\n",
      "processing patch 1 of 6\n",
      "processing patch 2 of 6\n",
      "processing patch 3 of 6\n",
      "processing patch 4 of 6\n",
      "processing patch 5 of 6\n",
      "processing patch 6 of 6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Max value == min value, ambiguous given dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6f6096c41731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxon_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquired_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/axondeepseg/AxonDeepSeg/apply_model.py\u001b[0m in \u001b[0;36maxon_segmentation\u001b[0;34m(path_acquisitions_folders, acquisitions_filenames, path_model_folder, config_dict, ckpt_name, segmentations_filenames, inference_batch_size, overlap_value, resampled_resolutions, acquired_resolution, prediction_proba_activate, write_mode, gpu_per, verbosity_level)\u001b[0m\n\u001b[1;32m    261\u001b[0m                         \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_acquisitions_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentations_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                         \u001b[0maxon_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyelin_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_acquisitions_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegmentations_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprediction_proba_activate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/axondeepseg/AxonDeepSeg/visualization/get_masks.py\u001b[0m in \u001b[0;36mget_masks\u001b[0;34m(path_prediction)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtmp_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_seg-axon.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxon_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_seg-myelin.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyelin_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads3.6/lib/python3.6/site-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimwrite\u001b[0;34m(uri, im, format, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# Return a result if there is any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads3.6/lib/python3.6/site-packages/imageio/core/format.py\u001b[0m in \u001b[0;36mappend_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;31m# Call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mset_meta_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads3.6/lib/python3.6/site-packages/imageio/plugins/pillow.py\u001b[0m in \u001b[0;36m_append_data\u001b[0;34m(self, im, meta)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_as_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbitdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_as_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbitdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0mPillowFormat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ads3.6/lib/python3.6/site-packages/imageio/core/util.py\u001b[0m in \u001b[0;36mimage_as_uint\u001b[0;34m(im, bitdepth)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Maximum image value is not finite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max value == min value, ambiguous given dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         _precision_warn(dtype_str1, dtype_str2,\n\u001b[1;32m    113\u001b[0m                         'Range [{}, {}].'.format(mi, ma))\n",
      "\u001b[0;31mValueError\u001b[0m: Max value == min value, ambiguous given dtype"
     ]
    }
   ],
   "source": [
    "# Set paths (comment for using model trained at section 1.)\n",
    "path_folder, file_name = os.path.split(path_img)\n",
    "\n",
    "# reset the tensorflow graph for new testing\n",
    "tf.reset_default_graph()\n",
    "\n",
    "prediction = axon_segmentation(path_folder, file_name, path_model, config_network, acquired_resolution=0.1, verbosity_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Load the segmented image and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img_path = '/Users/Documents/Aldo/ads_feb/axondeepseg/AxonDeepSeg/data_test/AxonDeepSeg.png'\n",
    "pred_img = imageio.imread(pred_img_path)\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "plt.title('Prediction with the trained model')\n",
    "plt.imshow(pred_img,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
